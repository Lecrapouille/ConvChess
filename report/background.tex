\chapter{Background}
\label{chap:background}
In this chapter, we will first look at what it means to actually solve the game 
of chess, before moving on to studying how computers are programmed to play 
chess efficiently and exceptionally well. Further we look at some background 
about reinforcement learning, deep learning and 
learning representations and how 
convolutional neural networks, a specific case of deep learning architectures, 
has shaped the field of artificial intelligence especially in the area of 
pattern recognition.% in~\ref{subsection:cnn-background}. 
% In section~\ref{subsection:previous-works}, we also introduce some effective 
% approaches that solve problems similar to the task at hand like playing Atari 
% games using deep reinforcement learning, predicting moves in the games of Go and 
% evaluating moves in chess.  
\section*{Solving Chess}
\label{section:solving}
Solving chess refers to finding an optimal strategy for playing chess. Chess has 
a finite number of states, estimated to be around $10^{43}$ 
by \citet{shannon1950xxii}. \citet{zermelo1913anwendung} proved that a 
hypothetically determinable optimal strategy does exist for chess.\\
In a weaker sense, each position can be assigned as a win for white, a win for 
Black or a forced draw if both the players are using the optimal strategy. We 
can formulate this as a function $f$ for each board position using the procedure 
below.
\begin{enumerate}
\item Assign all the positions with no further play possible as:
\[f(position) = \begin{cases}
1 \text{ , if White has won}\\
0 \text{ , if it is a draw}\\
-1 \text{ , if Black has won}
\end{cases}\]
\item Use the recursive rule up the game tree:
\[f(p) = \max\limits_{p\rightarrow p'} (-f(p'))\]
where $p'$ is a position reachable in one move from position $p$.\\
Here the negative sign means that a win for the opponent is a loss for the 
player in consideration and vice-versa.
\end{enumerate} 
The above recursive rule is the same as the minimax algorithm for the case when 
the game tree for chess is fully grown. The function $f$ gives us the 
``perfect'' evaluation function to play a chess game. While playing the game, we 
just need to choose the move which takes us to the board position $p$ for which 
$f(p)=1$.\\

But this evaluation function, $f$, cannot be computed with the computing 
resources available as of now. Hence, we need to resort to approximations of 
$f(p)$.


\section*{Playing Chess using Computers}
\label{section:playing-background}
The first study on computers playing the game of chess was done by 
\citet{shannon1950xxii}. He predicted two main search strategies that would 
evolve with computers being programmed to play chess--
\begin{enumerate}
\item ``Type A" programs would use a ``brute force" examination of all the board 
positions possible through valid moves upto a certain depth of play.
\item ``Type B" programs would employ a ``quiescence search" looking at only a 
few good moves for each position. 
\end{enumerate}
He also predicted the ``Type A" programs to be impractical because of the 
massive search space. For even a computer evaluating $10^6$ moves per second, a 
lookahead of 3 moves for both sides, when an average of 30 moves are available 
for each board position, the program would take around 13 minutes.\\
\begin{table}[h]
\centering
%\resizebox{\textwidth}{!}{%
\begin{tabular}{lll}
\toprule
Depth (ply) & Number of Positions & Time to Search \\ 
\midrule
2           & $900$                 & 0.0009 s       \\ 
3           & $2.7\times10^3$       & 0.027 s        \\ 
4           & $8.1\times10^4$       & 0.81 s         \\ 
5           & $2.43\times 10^7$    & 24.3 s         \\ 
6           & $7.29\times 10^9$    & 12.15 mins     \\ 
7           & $2.187\times 10^{10}$& 6.075 hours    \\ \bottomrule
\end{tabular}
%}
\caption{Time taken to explore 30 moves per position at $10^6$ moves per 
second.}
\label{table:time-taken}
\end{table}

However, with the exponential increase in the computation power since then, the 
most successful methods for playing chess are programmed to use ``Type A" 
programs. One reason for ignoring ``Type B'' is that relying on the board 
configuration to decide on the moves to explore is a much harder problem than 
using a faster, yet weaker, evaluation metric for a larger depth. It is widely 
believed that a faster evaluation function with an efficient search along with 
heuristic pruning would build a better chess playing computer than the one that 
uses a better but slower evaluation function that involves pattern recognition 
techniques similar to our brain.

\subsection*{Conventional Chess Playing Computers}
\label{subsection:conventional-chess}
The fundamental implementation details of chess-playing computer system include:
\begin{itemize}
\item \textbf{Board Representation} -- how a board position is represented in a 
data structure. The performance of move generation and piece evaluation depends 
on the data structure used to represent the board position. The most common 
representation uses a list of piece positions for each position. Some of the 
other methods are-- mailbox, 0x88, bitboards and huffman codes. We will discuss 
the representation used for our training and playing tasks in Chapter 
\ref{chap:dataset}.

\item \textbf{Search Techniques} -- how to identify and select the moves for 
further evaluation. Some of the most widely used search techniques are-- 
Minmax, Negamax, Negascout, Iterative deepening depth-first search etc. Much of 
the 
focus on state of the art chess playing systems resides on making search more 
efficient to evaluate more board positions per unit time. One of these 
algorithms which we make use of is Negamax algorithm and has 
been explained in section~\ref{subsection:interleaved}.

\item\textbf{Leaf Evaluation} -- how to evaluate the position of the board if 
no 
further evaluation needs to be done. The mapping from a board to an integer 
value is called the evaluation function. An evaluation function typically 
considers material value along with other factors affecting the strength of 
play for each player. The most common values for materials is 1 point for pawn, 
3 for bishop, 3 for knight, 5 for rook, 9 for queen and 200 for king. The high 
value for king ensures that checkmate outweighs everything. The sum of the 
values for all material on the board with negative weights to the opponent is 
the evaluation of the table. In addition to pieces, most evaluation functions 
take into consideration more factors like pawn structure, pair of bishops, 
protection of the king etc.
\end{itemize}


\section*{Reinforcement Learning}
\label{section:RL}
The fundamental principle behind all Reinforcement learning methods is that we 
use the current policy, run it on the environment, observe the feedback and 
make the good outcomes more likely, the bad outcomes less likely. Reinforcement 
learning differs from standard supervised learning in the way that an RL 
algorithm is not presented with optimal actions to input states 
\cite{sutton1998reinforcement}. The basic reinforcement learning model contains 
the following components:
\begin{enumerate}
 \item a set of environment states $\mathcal{S}$
 \item a set of actions $\mathcal{A}$
 \item transition rules between states
 \item rules that determine the \textit{scalar intermediate reward} of a 
transition
\item rules that describe what the agent observes
\end{enumerate}
However it is a very hard problem to do a policy search using a reinforcement 
learning architecture when we have a combination of complex dynamics and 
complex policy. The high dimensionality of such policies doesn't allow 
efficient policy search. However, there have been successful attempts to 
convert these complex dynamics and complex policy domain problems to only a 
complex policy problem by decomposing the policy search into two phases-- 
optimal control and supervised learning \cite{levine2015end}. The end to end 
training for the supervised learning part is done using a function 
approximator, convolutional neural networks in our case, which we will 
discuss in section~\ref{subsection:cnn-background}. 
Other attempts to combine conventional reinforcement learning techniques with 
deep learning to learn optimal control have yielded near human performance on 
tasks like playing Atari video games \cite{deepmind_nips}. The task that 
representation learning architectures like CNNs solve is the problem of 
perception i.e. the feature representation of the states need not be 
composed of hand-crafted features, but can be learned while training.

\section*{Deep Learning}
\label{section:dl-background}
The area of Neural Networks has been inspired by the aim of modeling biological 
neural systems. Although it has diverged from this aim since then, the basic 
computational unit in an artificial neural network still remains a neuron, 
which takes the signal from the axons from other neurons as inputs, 
with dendrites carrying the signal to the nucleus where it gets summed up and 
the neuron is activated if the sum is above some threshold. Such a neuron can 
be represented mathematically as: $f(\sum_i w_ix_i + b)$, where $f$ is the 
activation function, $w_i$ is the weight given to the input from one of its 
dendrites. In other words, a neuron computes the dot product of its weights and 
the inputs, adds the bias and applies an activation function. A mathematical 
model of a neuron is shown in the figure below.
\begin{figure}[H]
\begin{tikzpicture}[
init/.style={
  draw,
  circle,
  inner sep=2pt,
  font=\Huge,
  join = by -latex
},
squa/.style={
  draw,
  inner sep=2pt,
  font=\Large,
  join = by -latex
},
start chain=2,node distance=13mm
]
\node[on chain=2] 
  (x2) {$x_2$};
\node[on chain=2,join=by o-latex] 
  {$w_2$};
\node[on chain=2,init] (sigma) 
  {$\displaystyle\Sigma$};
\node[on chain=2,squa,label=above:{\parbox{2cm}{\centering Activate \\ 
function}}]   
  {$f$};
\node[on chain=2,label=above:Output,join=by -latex] 
  {$y$};
\begin{scope}[start chain=1]
\node[on chain=1] at (0,1.5cm) 
  (x1) {$x_1$};
\node[on chain=1,join=by o-latex] 
  (w1) {$w_1$};
\end{scope}
\begin{scope}[start chain=3]
\node[on chain=3] at (0,-1.5cm) 
  (x3) {$x_3$};
\node[on chain=3,label=below:Weights,join=by o-latex] 
  (w3) {$w_3$};
\end{scope}
\node[label=above:\parbox{2cm}{\centering Bias \\ $b$}] at (sigma|-w1) (b) {};

\draw[-latex] (w1) -- (sigma);
\draw[-latex] (w3) -- (sigma);
\draw[o-latex] (b) -- (sigma);

\draw[decorate,decoration={brace,mirror}] (x1.north west) -- node[left=10pt] 
{Inputs} (x3.south west);
\end{tikzpicture}
\caption{An artificial neuron as a mathematical model}
\end{figure}

Neural networks are the collections of neurons that are connected in an acyclic 
graph. This means that outputs of some set of neurons becomes the input of 
another set of neurons. The most common arrangement is a layered neural network 
with an input and an output layer, along with none or more hidden layers. 
The input layer has number of neurons equal to the input dimension and the 
number of output layer neurons is the dimension of the output. The hidden 
layers can contain different numbers of neurons. A two-layer neural network is 
shown in the figure below. The two-layer here refers to the number of layers 
besides the input layer.
\begin{figure}[H]
\begin{tikzpicture}[
plain/.style={
  draw=none,
  fill=none,
  },
net/.style={
  matrix of nodes,
  nodes={
    draw,
    circle,
    inner sep=10pt
    },
  nodes in empty cells,
  column sep=2cm,
  row sep=-9pt
  },
>=latex
]
\matrix[net] (mat)
{
|[plain]| \parbox{1.3cm}{\centering Input\\layer} & |[plain]| 
\parbox{1.3cm}{\centering Hidden\\layer} & |[plain]| \parbox{1.3cm}{\centering 
Output\\layer} \\
& |[plain]| \\
|[plain]| & \\
& |[plain]| \\
|[plain]| & |[plain]| \\
& & \\
|[plain]| & |[plain]| \\
& |[plain]| \\
|[plain]| & \\
& |[plain]| \\
};
\foreach \ai [count=\mi ]in {2,4,...,10}
  \draw[<-] (mat-\ai-1) -- node[above] {Input \mi} +(-2cm,0);
\foreach \ai in {2,4,...,10}
{\foreach \aii in {3,6,9}
  \draw[->] (mat-\ai-1) -- (mat-\aii-2);
}
\foreach \ai in {3,6,9}
  \draw[->] (mat-\ai-2) -- (mat-6-3);
\draw[->] (mat-6-3) -- node[above] {Ouput} +(2cm,0);
\end{tikzpicture}
\caption{A two layer Artificial Neural network}
\end{figure}

In practice, we model a real valued function using a multi-layer neural 
network. This real valued function, for example, is the one which outputs the 
class value in case of a classification task or a continuous function in case 
of a regression task. In other words, a multi-layer artificial neural 
network defines a family of functions parameterized by the weights of the 
network. By learning the weights of such a network we usually means learning a 
function belonging to this family of functions that best represents the training 
data.

\subsection*{Universal Approximation Properties of Multilayer Perceptrons}
\label{subsection:universal-approx}
We discussed above that a family of functions is represented by a single 
artificial neural network with fixed architecture. It has been proved that 
Neural networks with at least one hidden layer are universal 
approximators \cite{hornik1989multilayer}. This means that given any continuous 
function $f(x)$ and some $\epsilon>0$, a Neural network with one hidden layer 
containing a sufficient number of hidden layer neurons and a suitable choice of 
non-linearity, say represented by $g(x)$, exists such that $\forall x, 
|f(x)-g(x)|<\epsilon$. In other words, we can approximate any given real 
valued continuous function with a two layer Neural network upto a certain 
accuracy.\\

The fact that two layer Neural networks are universal approximators 
is a pretty useless property in the case of machine learning. Neither does it 
tell the number of hidden units required to represent a given function upto the 
desired precision, nor does it promise that it represents a generalized function 
that fits the unseen data. The generalized function is expected to be smooth, 
while the overly precise two-layer network may overfit for the input data and 
not learn a promising representation.

\subsection*{Representation Learning}
We learnt that despite being universal approximators, it may not reasonable to 
approximate a function for a task at hand using two-layer Neural networks 
because of insufficient generalization ability. Meanwhile, a lot of 
experimental evidence from the recent past shows that these functions can be 
learnt to a greater generalization using Neural networks of larger depths 
\cite{bengio-dl-book}. Most of these works involve using a specific class of 
Neural networks architectures, namely Convolutional Neural Networks, which we 
will discuss in section \ref{subsection:cnn-background}. This leads us to 
believe that depth indeed is a useful consideration to make while choosing a 
Neural network architecture to fit data, and it provides the learner's family 
of functions multiple levels of representation and hence making the function 
smoother yielding better generalization. 

\subsection*{Convolutional Neural Networks}
\label{subsection:cnn-background}
Convolutional neural networks, sometimes referred to as Convolutional Networks 
or ConvNets, is a particular architecture of deep neural networks inspired 
by the seminal work by \citet{hubel1963shape} on feedforward processing in 
early visual cortex. The architecture uses hierarchical layers of tiled 
convolutional filters to mimic the effects of receptive fields. These filters 
exploit the local spatial correlations present in the images. In 
practice, these hierarchical layers are alternated with subsampling layers like 
max-pool and a non-linearity map, and further connected to fully connected 
layers just like in other deep learning architectures and the full 
network is trained using back propagation. In short, Convolutional neural 
networks are nothing but neural networks which use convolution instead of 
full matrix multiplications in atleast one of the layers \cite{bengio-dl-book}. 
\\
\begin{figure}[H]
\centering
\begin{tikzpicture}

				\node at (0.5,-0.75){\begin{tabular}{c}input 
image\end{tabular}};
		
				\draw (0,0) -- (1,0) -- (1,1) -- (0,1) -- (0,0);
		
				\node at 
(3,3.25){\begin{tabular}{c}convolutional layer\\with 
non-linearities\end{tabular}};
		
				\draw[fill=black,opacity=0.2,draw=black] 
(2.75,1.25) -- (3.75,1.25) -- (3.75,2.25) -- (2.75,2.25) -- (2.75,1.25);
				\draw[fill=black,opacity=0.2,draw=black] (2.5,1) 
-- (3.5,1) -- (3.5,2) -- (2.5,2) -- (2.5,1);
				\draw[fill=black,opacity=0.2,draw=black] 
(2.25,0.75) -- (3.25,0.75) -- (3.25,1.75) -- (2.25,1.75) -- (2.25,0.75);
				\draw[fill=black,opacity=0.2,draw=black] (2,0.5) 
-- (3,0.5) -- (3,1.5) -- (2,1.5) -- (2,0.5);
				\draw[fill=black,opacity=0.2,draw=black] 
(1.75,0.25) -- (2.75,0.25) -- (2.75,1.25) -- (1.75,1.25) -- (1.75,0.25);
				\draw[fill=black,opacity=0.2,draw=black] (1.5,0) 
-- (2.5,0) -- (2.5,1) -- (1.5,1) -- (1.5,0);
		
				\node at 
(4.5,-0.75){\begin{tabular}{c}subsampling layer\end{tabular}};
		
				\draw[fill=black,opacity=0.2,draw=black] 
(5,1.25) -- (5.75,1.25) -- (5.75,2) -- (5,2) -- (5,1.25);
				\draw[fill=black,opacity=0.2,draw=black] 
(4.75,1) -- (5.5,1) -- (5.5,1.75) -- (4.75,1.75) -- (4.75,1);
				\draw[fill=black,opacity=0.2,draw=black] 
(4.5,0.75) -- (5.25,0.75) -- (5.25,1.5) -- (4.5,1.5) -- (4.5,0.75);
				\draw[fill=black,opacity=0.2,draw=black] 
(4.25,0.5) -- (5,0.5) -- (5,1.25) -- (4.25,1.25) -- (4.25,0.5);
				\draw[fill=black,opacity=0.2,draw=black] 
(4,0.25) -- (4.75,0.25) -- (4.75,1) -- (4,1) -- (4,0.25);
				\draw[fill=black,opacity=0.2,draw=black] 
(3.75,0) -- (4.5,0) -- (4.5,0.75) -- (3.75,0.75) -- (3.75,0);
		
% %				\node at (7,3.5){\begin{tabular}{c}convolutional 
% layer\\with non-linearities\\layer $l = 4$\end{tabular}};
% %		
% %				\draw[fill=black,opacity=0.2,draw=black] 
% (7.5,1.75) -- (8.25,1.75) -- (8.25,2.5) -- (7.5,2.5) -- (7.5,1.75);
% %				\draw[fill=black,opacity=0.2,draw=black] 
% (7.25,1.5) -- (8,1.5) -- (8,2.25) -- (7.25,2.25) -- (7.25,1.5);
% %				\draw[fill=black,opacity=0.2,draw=black] 
% (7,1.25) -- (7.75,1.25) -- (7.75,2) -- (7,2) -- (7,1.25);
% %				\draw[fill=black,opacity=0.2,draw=black] 
% (6.75,1) -- (7.5,1) -- (7.5,1.75) -- (6.75,1.75) -- (6.75,1);
% %				\draw[fill=black,opacity=0.2,draw=black] 
% (6.5,0.75) -- (7.25,0.75) -- (7.25,1.5) -- (6.5,1.5) -- (6.5,0.75);
% %				\draw[fill=black,opacity=0.2,draw=black] 
% (6.25,0.5) -- (7,0.5) -- (7,1.25) -- (6.25,1.25) -- (6.25,0.5);
% %				\draw[fill=black,opacity=0.2,draw=black] 
% (6,0.25) -- (6.75,0.25) -- (6.75,1) -- (6,1) -- (6,0.25);
% %				\draw[fill=black,opacity=0.2,draw=black] 
% (5.75,0) -- (6.5,0) -- (6.5,0.75) -- (5.75,0.75) -- (5.75,0);
% %		
% %				\node at (9.5,-1){\begin{tabular}{c}subsampling 
% layer\\layer $l = 6$\end{tabular}};
% 		
% %				\draw[fill=black,opacity=0.2,draw=black] 
% (10,1.75) -- (10.5,1.75) -- (10.5,2.25) -- (10,2.25) -- (10,1.75);
% %				\draw[fill=black,opacity=0.2,draw=black] 
% (9.75,1.5) -- (10.25,1.5) -- (10.25,2) -- (9.75,2) -- (9.75,1.5);
% %				\draw[fill=black,opacity=0.2,draw=black] 
% (9.5,1.25) -- (10,1.25) -- (10,1.75) -- (9.5,1.75) -- (9.5,1.25);
% %				\draw[fill=black,opacity=0.2,draw=black] 
% (9.25,1) -- (9.75,1) -- (9.75,1.5) -- (9.25,1.5) -- (9.25,1);
% %				\draw[fill=black,opacity=0.2,draw=black] 
% (9,0.75) -- (9.5,0.75) -- (9.5,1.25) -- (9,1.25) -- (9,0.75);
% %				\draw[fill=black,opacity=0.2,draw=black] 
% (8.75,0.5) -- (9.25,0.5) -- (9.25,1) -- (8.75,1) -- (8.75,0.5);
% %				\draw[fill=black,opacity=0.2,draw=black] 
% (8.5,0.25) -- (9,0.25) -- (9,0.75) -- (8.5,0.75) -- (8.5,0.25);
% %				\draw[fill=black,opacity=0.2,draw=black] 
% (8.25,0) -- (8.75,0) -- (8.75,0.5) -- (8.25,0.5) -- (8.25,0);
% 		
				\node at (6.5,1){$\ldots$};
		
				\node at (8.5,3.25){\begin{tabular}{c}two-layer 
perceptron\end{tabular}};
		
				\draw[fill=black,draw=black,opacity=0.5] (6.5,0) 
-- (7,0) -- (8.5,1.75) -- (8,1.75) -- (6.5,0);
		
% 				%\node at (9,-1){\begin{tabular}{c}fully 
% connected layer\\output layer $l = 8$\end{tabular}};
		
				\draw[fill=black,draw=black,opacity=0.5] 
(8.5,0.5) -- (9,0.5) -- (9.65,1.25) -- (9.15,1.25) -- (8.5,0.5);
\end{tikzpicture}
\caption{A typical Convolutional Neural Network architecture}			
\end{figure}		
			
Convolutional Neural Networks have brought about a revolution in computer 
vision and is now the most successful approach for almost all recognition and 
detection tasks in computer vision 
\cite{krizhevsky2012imagenet,tompson2014efficient,taigman2014deepface} 
and some even approach human performance on some tasks 
\cite{ciresan2012multi}.\\

% \subsection{Related work}
% In the upcoming subsections, we will look at some of the related works in the 
% fields of learning games using machine learning rather than logic and search. 
% Some of the very recent works in combining deep learning with 
% reinforcement learning has resulted in human level control of arcade 
% games~\ref{subsubsection:deepmind}. Many of these works have inspired us to 
% take up Chess playing as machine learning and pattern recognition task. 
% \label{subsection:previous-works}
% \subsubsection{Convolutional Neural Networks for playing Go}
% 
% Following the work of \citet{sutskever2008mimicking} which achieved modest 
% success due to relatively small sized architecture, \citet{maddison2014move} 
% used deep convolutional neural networks to play Go. The network could predict 
% the correct move 55\% of the time and beat the traditional search program 
% GnuGo in 97\% of the games without using any search and match the performance 
% of a state-of-the-art Monte-Carlo tree search that simulates a million 
% positions per move. They represent the current board position as a 3 channel 
% image, along with adding channels for number of liberties before and after 
% move, legality and the rank of the player. Much of the formulation of our 
% model is motivated by this representation.\\
% 
% This work also inspired a modest attempt of using convolutional neural networks 
% to play chess, which achieved minimal success because of a much small dataset 
% and weak representation \cite{oshripredicting}. We also acknowledge this work to 
% have inspired us to make use of Convolutional neural networks to play chess. 
% Other works that use deep learning to play chess do not learn the piece 
% arrangements and their relative importances to score the table or evaluate it, 
% rather they utilize a set of handcrafted features like king's safety, king's 
% liberty, number of rooks on seventh rank etc. \cite{mannen2003learning, 
% thrun1995learning} \\
% 
% \textbf{Go versus Chess}\\
% Using Convolutional networks to predict moves in Go proves to be a great step 
% in the direction of building state of the art Go systems. This motivates us to 
% use the same technique in predicting moves in the game of Chess. However, we 
% must realize that the two games are fundamentally different in many aspects 
% that make Go an easier game to play using a Convolutional Network. The game of 
% Go has smoother arrangements of positions that are almost continuous 
% within and between games.\\
% 
% Every move in Go adds a single piece to the board. 
% Numerically speaking, making a move in Go using a random draw has a chance of 
% $\frac{1}{361}$ on a $19\times 19$ board, while making a move drawing 
% randomly the from and to positions in chess is correct with a 
% chance of a mere $\frac{1}{4096}$. Also, a single move on a chess board makes a 
% change of at least 2 pixels (more than 2 for a piece capture) which is 
% significant for an $8\times 8$ board, while in Go only 1 pixel is added to the 
% board every move making the transition much smoother than chess. Since, much 
% of the knowledge of chess is characterized by strong domain knowledge in a 
% logical form, such as ``if bishop on the central diagonal'', ``if the king has 
% liberty more than 1'' etc., makes it less intuitive if as compared to the case 
% of Go, Convolutional Neural networks can actually model the rules, leave 
% aside the optimality of the moves.\\
% 

%XXX Describe this work in greater detail.

% \subsubsection{Playing Arcade Games using Convolutional Neural Networks}
% \label{subsubsection:deepmind}
% A remarkable work that helped bridge the gap between perception and action has 
% been Deepmind's deep reinforcement learning agent that is capable of learning 
% human-level control from high dimensional sensory input of an Atari video game 
% \cite{deepmind_nips}.\\
% 
% We tried to explore the last fully connected layer of a 
% convolutional neural network trained to play Breakout. The 
% figure~\ref{figure:deepmind} shows the apparent separation between the state 
% embeddings which are mapped to different actions.\\
% 
% \begin{figure}[h]
%  \centering
%  \hspace*{-1.2in}
%  \includegraphics[width=1.5\textwidth]{plots/tsne_breakout_new.pdf}
%  
%  \caption[Fully connected layer's TSNE embedding in Deepmind's network]{The 
% plot shows the TSNE embedding of the last fully connected layer in a network 
% trained for the game of Breakout. The two primary colors--green and brown, 
% respectively, show the two primary actions learned while playing the game of 
% Breakout i.e. left and right respectively and the separation is almost 
% apparent. }
% \label{figure:deepmind}
% \end{figure}
% 
% 
% An extension of this work by \citet*{guo2014deep} 
% improved the results a bit on some games that required extensive planning. They 
% used a Monte-Carlo tree search based player to simulate trajectories, before 
% using the generated state-action-value instances to train a CNN based 
% regression model. The method proved to make a fruitful integration of a 
% planning agent and the RL agent in a way similar to the one discussed 
% in section~\ref{section:RL}.\\ 
% 
% Along with the TSNE embedding in figure~\ref{figure:deepmind}, builds our 
% intuition that the subspace that the convolutional neural network embeds the 
% state representations (i.e. the game screens) in Breakout on the basis of the 
% most favorable action for that state. We will see that 
% regardless of the task specification i.e. either we choose Q-learning (e.g. in 
% \citet{deepmind_nips}) or we choose a regression or classification task (e.g. 
% in \citet{guo2014deep}), we are almost asking the network to embed the states 
% effectively to select the best action.\\
% 
% This is the major motivation to our method of learning the evaluation 
% function for chess boards (state) based on the final outcome of the games, 
% which we describe in the next chapter.