\chapter{Implementation}
\label{chap:implementation}
The major contribution of our work is to provide a method in which 
convolutional neural networks can be trained to learn and play the game of 
chess. In this chapter, we will describe first discuss the details of the 
architectures we use (section~\ref{section:network-arch}), before moving on the 
algorithms to decide which move to play at the current board position 
(section~\ref{section:playing}).

\section{Piece and Move Predictor}
\label{section:network-arch}
\begin{figure}[h]
\centering
\includegraphics[scale=0.75]{img/from_to.png}
\caption{Model Overview}
\label{figure:model_overview}
\justifying
\small The green circle is the position to be predicted by the \textsc{Piece} 
selector network, while the red circle is the position to be predicted by the 
\textsc{Bishop} move selector network.
\end{figure}
A move in chess is characterized by legal movement of a friendly piece. The 
selected piece is moved onto one of the positions depending on factors like its 
ability to jump over another piece or not, whether the final position already 
has a friendly piece, whether the move doesn't cause a check etc. We divide 
this task into two simple steps--choose a piece, choose its final position. 
Given a board, we predict the next move by simply making the 
predictions in this order. While learning we consider the moves from the 
dataset as gold standard and minimize our loss functions in order to make the 
network learn certain generalizations, rules and even strategies. In 
figure~\ref{figure:model_overview}, the two different colored circles represent 
these two tasks. A formulation of this process would be:
\[P(move | board) = P(from|board)\times P(to|from, board)\]
To learn the probability function, we make use of Convolutional Neural 
Networks. Each of the different types of pieces has its own independently 
trained network, namely-- \textsc{Pawn}, \textsc{Rook}, \textsc{Knight}, 
\textsc{Bishop}, \textsc{Queen} and \textsc{King}, while the piece selector 
network is named \textsc{Piece}.\\

The network used for both piece selector and move selectors are shown below:\\
\begin{figure}[H]
\includegraphics[width=1.0\textwidth,center]{img/net1.png}
\caption{Network Architecture: ChessNet-1}
\small\centering
This network was used for piece and move predictions.
\label{figure:network1}
\end{figure}
The convolutional neural network shown in figure~\ref{figure:network1} consists 
of input layer of size $8\times 8\times 6$ (or $8\times 8\times 12$ depending 
on the choice of representation, described in 
section~\ref{section:representation12}), an output layer of size 64 which 
predicts the position of the piece to be moved. The other specifications of the 
network are:
\begin{itemize}
 \item \textbf{Convolutional Layer}-- 
 \begin{enumerate}
  \item filter size =$3\times 3$ each of depth 6
  \item number of filters=$64$
  \item padding=1
 \end{enumerate}
 \item Non-linearity-- \textbf{ReLU} $max(0,x)$
 \item \textbf{Convolutional Layer}-- 
 \begin{enumerate}
  \item filter size =$3\times 3$ each of depth 64
  \item number of filters=$96$
  \item padding=1
 \end{enumerate}
 \item Non-linearity-- \textbf{ReLU} $max(0,x)$
 \item \textbf{Convolutional Layer}-- 
 \begin{enumerate}
  \item filter size =$3\times 3$ each of depth 96
  \item number of filters=$256$
  \item padding=1
 \end{enumerate}
 \item Non-linearity-- \textbf{ReLU} $max(0,x)$
 \item \textbf{Fully connected layer}-- dimensions=1024
 \item Softmax (Loss) Layer -- \textbf{Softmax}, 
$output_k=\frac{e^{x_k}}{\sum_i e^{x_i}}$, $x_k$ is the activation at the last 
fully connected layer in the network.
\end{itemize}

While training, the softmax layer is replaced by a softmax loss layer, output 
for the softmax loss layer is defined as follows:
\[L = - \sum_j y_j log p_j\]
where L is the loss, $y_j$ is 0 or 1 depending on the actual 
label.\label{section:lossfunc} The 
derivative of the loss function can be derived as follows:
\begin{align*}
 \frac{\partial L}{\partial o_i} &= -\sum_k y_k\frac{\partial log p_k}{\partial 
o_i} \\
&= -\sum_k y_k \frac{1}{p_k}\frac{\partial p_k}{\partial o_i}\\
&= -y_i(1-p_i) - \sum_{k\neq i} y_k \frac{1}{p_k}(-p_kp_i)\\
&= p_i\bigg(\sum_k y_k\bigg) - y_i\\
&= p_i-y_i
\end{align*}

\section{Learning the evaluation function}
\label{section:eval-func}
In addition to a piece and move predictor, we also learn an evaluation 
function. Since we want to be consistent with not injecting any domain 
knowledge for the game of chess to our models, we formulate the task of 
learning the evaluation function as regression problem with the evaluations of 
the boards inspired by TD-Learning \cite{Barto89learningand, 
sutton1988learning}. Particularly, without using any other knowledge of the 
game, we simply assign discounted reward values to the board as its evaluation. 
Starting with the final leaf nodes (after which no more gameplay is possible or 
one of the player has resigned), we assign board values to be 1 for the winning 
player's board view, -1 for the losing player's board 
view\footnote{Since we are learning from white's perspective, the player's 
board view refers to how the board would have looked to the player if he was 
playing as a white i.e. the board flipped across the horizontal as well 
flipped by color.} and 0 for both if it was a draw. This assignment is the same 
as that 
described as an optimal evaluation function for chess given the fully expanded 
game tree described in~\ref{section:solving}. But for the preceding board 
positions, we assign a discounted reward value as the valuation using a 
discounting factor represented by $\gamma$, where $0\leq \gamma \leq 1$.
\[V(b_{t_{final}}) = \begin{cases}
1 \text{ , if White has won}\\
0 \text{ , if it is a draw}\\
-1 \text{ , if Black has won}
\end{cases}\]
According to the recursive rule the discounted reward for a board at time $t$ 
into the game is:
\[V(t) = \gamma V(t+1) , \forall t<t_{final}\]
Use the following rule moving up into the game:
\[V(b_{t_{final}-i}) = \begin{cases}
		\gamma^{i} \text{ , if white won eventually}\\
                -\gamma^{i} \text{ , if black won eventually}\\
                0 \text{ , if the game was a draw}\\
               \end{cases}\]
\[V(b'_{t_{final}-i}) = \begin{cases}
		-\gamma^{i} \text{ , if white won eventually}\\
                \gamma^{i} \text{ , if black won eventually}\\
                0 \text{ , if the game was a draw}\\
               \end{cases}\]
where $b_{t_{final}-i}$ is the board (as it appears to the white player) $i$ 
steps away from the finish, while $b'_{t_{final}-i}$ is the rotated board with 
flipped colors i steps away from the finish.\\

Since the learning is done offline (in our case using game databases), we 
simulate the whole game before assigning values to each board appearing in the 
board. Now, each board position seen in the data has some evaluation that is 
based on the final outcome of the game. It is intuitive to say that a player who 
is seeing a higher valued board is more likely to win. And higher the value of 
the current board, the closer I am to winning the game and vice versa.\\

We model the task of learning such an evaluation function as regression problem 
on the network shown in figure~\ref{figure:network2}.
\begin{figure}[h]
\includegraphics[width=1.0\textwidth,center]{img/net3.png}

\caption{Network Architecture: ChessNet-2}
\small\centering
This network was used for regression to learn the evaluation function. The 
difference from the network in~\ref{figure:network1}, other than the dimensions 
of output, is the type of non-linearity used. In this network we use $tanh$, 
rather than ReLU since we need values to range from -1 to 1. We also tried 
other non-linear activation functions like leaky ReLU or a sigmoid function 
with no significant difference in results.
\label{figure:network2}
\end{figure}

The input layer size is $6\times 8\times 8$, while the output layer size is 1 
i.e. the evaluation function's value for the input board. The loss function 
optimized to train the network is the \textit{mean squared error} which is the 
basic linear regression loss function.
\[L = \frac{1}{2}(f(x)-y)^2\] where x is the value at the last fully 
connected layer in the network and f is the activation function (tanh in our 
case).
Simply the gradient looks like:
\[\frac{\partial L}{\partial x} = (f(x)-y)\frac{\partial f(x)}{\partial x}\]

The backward pass for rest of the layers works in the same way as the previous 
network.

\section{Playing}
\label{section:playing}
In this section we will look at the algorithms we use during the gameplay to 
decide which move to play depending on the outputs of our models. The first two 
algorithms do not make use of search and just make predictions based on the 
current table, while the rest mix the predictions or evaluations made by our 
model with a search based mechanism to predict the next move.

\section{Choosing a move}
The output by each of the networks for a given input board (with or without the 
augmented channels) is a probability distribution over the whole board. The 
probability distribution output by the trained \textsc{Piece} network, call it 
$P_{piece}$, is a 64-dimensional vector where each value denotes the probability 
of moving the piece at the corresponding position on the board. Similarly, the 
probability distribution output by the $i^{th}$ \textsc{Move} network, call it 
$P_{move,i}$, is a 64-dimensional vector where each value denotes the 
probability of moving a piece of type $i$ to that particular position. We use 
two or more of these probabilities to choose the complete move (initial and the 
final position) using one of the methods described below.
\subsection{Top-move method}
Simply choose the piece at position with the maximum probability in the 
\textsc{Piece} network's output. Determine its piece type and input the same 
board to the type's \textsc{Move} network. Choose the final position with the 
maximum probability.
\begin{algorithm}[H]
\begin{algorithmic}[1]
 	\State initial\_pos = argmax($P_{piece}(board)$)\;
   	\State piece\_type = getType(initial\_pos)\;
   	\State final\_pos = \funccall{argmax}($P_{move,piece\_type}(board)$)\;
   	\Return chess\_coords(initial\_pos)+chess\_coords(	final\_pos)\;
\end{algorithmic}
\caption{Top-move method}
\label{alg:topmove}
\end{algorithm}

\subsection{Top-prob method}
\label{subsection:topprob}
Rather than just determining the distribution of the moves originating from the 
most probably piece, we can also generate the full cumulative probability 
distribution for every possible from-to pair. This method is based on the 
principle that: 
\[P(move | board ) = P(piece | board )\times 
P(final\_position|piece, board)\]

\begin{algorithm}[H]
 \begin{algorithmic}[1]
 	\State piece\_dist = $P_{piece}(board)$
   	\State cumulative\_dist = \funccall{zeros}(64,64)
   	\For {$0\leq i<64$}
   		\If {$board[i/8,i\% 8]\neq 0$}
   			\State piece\_type=\funccall{getType}(i)
   			\State move\_distr = $P_{move,piece\_type}(board)$ * 
piece\_distr[i]
   			\State cumulative\_distr[i] = move\_distr
   		\EndIf
   	\EndFor
   	\State initial\_pos, final\_pos = \funccall{argmax}(cumulative\_distr)
   	\Return \funccall{chess\_coords}(initial\_pos)+\funccall{chess\_coords}	
(	final\_pos)
 \end{algorithmic}
 \caption{Top-prob method}
 \label{alg:topprob}
\end{algorithm}

\subsection{TopProb-Negamax Interleaved}
\label{subsection:interleaved}
In this method we interleave our method of evaluating moves with negamax, which 
is a kind of minmax search algorithm. We will 
describe both Negamax and our pruning method interleaved with Negamax.
The basic idea is that we will reduce the search space for the Negamax 
algorithm and also provide it with an evaluation metric for the moves or the 
boards.\\
%XXX
The Negamax algorithm is derived from Minimax algorithm. However it differs in 
the way that it utilizes the same subroutine for the Min player and the Max 
player at each step, passing on the negated score following the rule:
\[max(a,b) = -min(-a,-b)\]
\begin{algorithm}[h]
 \begin{algorithmic}[1]
 	\Procedure{Negamax}(depth)
 	\If {depth==0} 
	  \Return \funccall{evaluate}()
 	\EndIf
 	\State max = $-\infty$
 	\State \funccall {generateMoves}($\dots$)
 	\While {m = \funccall{getNextMove}()}
	  \State \funccall{makeMove}(m)
	  \State score = -\funccall{Negamax}(depth -1)
	  \State \funccall{unmakeMove}(m)
	  \If {score $>$ max}
	    \State max = score
	  \EndIf
	\EndWhile
	\Return max
    \EndProcedure
 \end{algorithmic}
 \caption{The basic Negamax algorithm for Chess}
 \label{alg:negamax}
\end{algorithm}
 The functions $makeMove$ and $unmakeMove$, as the names suggest, make the move 
on the board before calling Negamax again for the child nodes, and then un-make 
the move to search the subtrees of the siblings.

However, the function calls of our interest is the $generateMoves$ and 
$evaluate$ function. $generateMoves$ function uses chess logic to generate 
legal moves, which are then explored using recursion, while $evaluate$ uses the 
players evaluation function to return the value for the node.\\

For our case, we can modify the $generateMoves$ procedure to actually just 
generate a reduced number of moves available at every board position, hence 
pruning the search tree. The reduced number of moves are generated in a way 
similar to algorithm~\ref{alg:topprob}, where we generate a 
cumulative probability distribution of size $64\times 64$. The difference being 
that rather than making a move, we just explore the subtree of the generated 
move. In the same way as before, we can make use of transposition tables to 
store the values of boards already evaluation while expansion of the tree.\\

Our idea of making a narrower and deeper search of the game tree instead of a 
full width and shallow search is motivated by \citealp*{de1996perception} as 
discussed in chapter~\ref{chap:background}.\\

For $evaluate$ function call, we simply ask our network to evaluate one or a 
batch of boards. The function is better described earlier in 
the section~\ref{section:eval-func}.

\section{Training: Implementation details and Hyper parameters}
\label{section:hyperparams}
We performed all our training and testing for the piece and move prediction 
models on a deep learning library named Caffe \cite{jia2014caffe} using its 
Python API. However, we couldn't use Caffe for the 
regression training and related experiments because of convergence issues. The 
implementation for the regression training to learn the 
evaluation function was done using a Theano-based deep learning library named 
Keras \cite{keras}. The complete source code of our implementation is available 
on \url{https://github.com/ashudeep/convchess}. \\

All the experiments were run on a machine with NVIDIA GeForce GTX 760 GPU with 
Cuda v6.5.\\

Move prediction models: We experimented with different hyperparameters to 
arrive at the following optimal hyper-parameters for the learning procedure and 
obtained our best models: base learning rate  of $\alpha=0.1$ and a batch 
size of 1000. We used AdaGrad to compute the updates. The learning rate was 
kept 
constant for the first $50,000$ iterations before reducing it to 0.01. The move 
prediction networks in Caffe were trained uptil $300,000$ iterations which took 
about 2-3 days.\\

Evaluation Function learning: We used a learning rate of 0.001 and RMSprop with 
a learning rate decay of 0.9 to minimize the mean squared error. We use a batch 
size of 1024. We varied the values of $\gamma$ (described in Chapter 
\ref{chap:implementation}) between 0.7 and 0.99 to train separate models. The 
results and analysis for each of them are described in this chapter.\\
