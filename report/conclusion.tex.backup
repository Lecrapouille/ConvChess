\chapter{Conclusion and Discussion}

We started with an aim to make a machine learn how to play chess giving it no 
or minimal prior knowledge. In the previous chapter, we saw a mannerly analysis 
of the results we obtain when convolutional neural networks are used to learn 
the game of chess. Although showing a promising behavior in various case 
studies performed, we saw that the gameplay is not very effective against a 
decent chess computer that uses heuristic search to choose a move. But, we 
believe that the work significantly bridges the gap between the cognitive 
aspects of chess playing and how the best chess computers play it. In this 
chapter, we discuss the contributions of our work and alongside try to 
ponder upon the shortcomings and discussing possible solutions.\\

\section{Positives}
We started with a motive to make a machine learn the game of chess with minimal 
prior knowledge and eventually have the ability to:
\begin{itemize}
 \item Learn to play legal moves
 \item Rank the possible moves without any explicit guidance on relative 
importance of material or position
 \item Evaluate board positions
\end{itemize}

Motivated to accomplish this task using minimal knowledge prior, we used 
convolutional neural network based architectures to learn-- the piece and move 
predictors as well as an evaluator. In the last section, we empirically 
demonstrated that the models learned the rules of the chess as well were able 
to make strong predictions on tasks where a novice could easily fail.



\section{Weaknesses}
We have already emphasized that our motivation was not to build a state of the 
art chess playing system, but to build a chess player from 
scratch i.e. having minimal prior knowledge. We saw in the last chapter that 
the gameplay of our system is not a championship level one. But it still could 
make strong predictions on certain tasks. However in this section, we will try 
to focus on the weaknesses of our models.\\

\subsection{Reasons}
The convolutional neural network trained on all the moves doesn't know if the 
given move at hand is a blunder or not. It can be the case that a common 
blunder is mistaken to be a favorable move in certain situations or if a board 
position is so rare that the only available supervision is not the best move to 
make in that situation.\\

However this can be explained to be considered while learning the evaluation 
function using discounted rewards. The problem with the evaluation function 
seems to be that the learning task is not very specific or well formulated. A 
visible weakness is the already present ambiguity in the training data for the 
network for instance the starting board, which looks the same for both the 
winning and the losing player, gets both a positive and a negative score. 
Although the value is small or can be made making a proper use of the decay 
parameter. 

\section{Further Work}
\subsection{Reinforcement Learning based player}
In chapter~\ref{chap:introduction}, we mentioned that it is a hard problem 
to 
do a policy search using a reinforcement learning architecture when we have a 
combination of complex dynamics and complex policy. Intuitively, why a 
reinforcement learning agent (RL agent) is a bad idea for chess is that a 
reward is received only once and after a long time. Hence, a learner who starts 
from scratch, using a random policy, would never reach the point when it 
receives a positive reward i.e. a win. There has to be a heuristic evaluation of 
the policy learned by such a system which allows it to update its policy. It is 
exactly at this point where the evaluation function learned from a large 
dataset of already played games can fit. The evaluation function (described 
in~\ref{section:eval-func}) computed at a certain ply depth can provide the 
board values as a feedback to the 
current policy of the RL agent.

\subsection{Evolving chess programs using Genetic Algorithms}
Another class of systems that use co-evolution, where the system learn by 
playing itself 
\cite{vazquez-coello-12_evolutionary_hooke-jeeves-algo_chess-evaluation}  and 
adjusting the parameters 
\cite{bovskovic-brest-11_tuning-chess-evaluation-w-differential-evolution}. A 
possible extension of this work could be to utilize such genetic programming 
architectures to improve the basic piece-move predictor presented in this 
thesis.


\section{Conclusion}
